{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y = b0 + b1*x1 + b2*x2 + b3*x3 ...\n",
    "```\n",
    "\n",
    "Feature scaling is not needed for multiple linear regression.\n",
    "\n",
    "#### Dummy variable trap\n",
    "\n",
    "Categorical variables (e.g city/country string in data which are converted to 0s and 1s columns) are considered as dummy variables for multiple linear regressions.\n",
    "\n",
    "The rule is to **never adding all** of the dummy variables int equation,  **always omit one**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"50_startups.csv\")\n",
    "# all the rows, all, the columns except the last one\n",
    "X = dataset.iloc[:, :-1].values\n",
    "# all the rows, all, only the last column\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# [-1] for encoding only the last columng of X (which is State)\n",
    "ct = ColumnTransformer(transformers=[(\"encoder\", OneHotEncoder(), [-1])], remainder=\"passthrough\")\n",
    "columns_before_transformation = len(X[0])\n",
    "X = np.array(ct.fit_transform(X))\n",
    "columns_after_transformation = len(X[0])\n",
    "encoded_columns_length = (columns_after_transformation - columns_before_transformation) + 1\n",
    "# print encode\n",
    "# print(X[:, :encoded_columns_length])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data set int the Training and Test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Multiple Linear Regression model on the Training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# dummy variable trap is avoided automatically\n",
    "# backward elemination technique etc, will also be applied automatically (features without   significant p values are filtered out)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X=X_train, y=y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Test result set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X=X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "# reshape in 2d array with single column\n",
    "y_pred_reshaped = y_pred.reshape(len(y_pred), 1)\n",
    "y_test_reshaped = y_test.reshape(len(y_test), 1)\n",
    "# 1 is for horizontal concat\n",
    "# 2d array  [ [pred, test], [pred, test], ... ], to compare\n",
    "y_pred_and_test_concat = np.concatenate((y_pred_reshaped, y_test_reshaped), axis=1 )\n",
    "print(y_pred_and_test_concat)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred, color=\"blue\")\n",
    "plt.plot(y_test, color=\"red\")\n",
    "plt.title(\"Estimations vs Actuals\")\n",
    "plt.xlabel(\"Estimations\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c5f496e35f44948724130b867049922961457e2749c2edaf845f04425f68df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
